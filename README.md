# ğŸŒ  Shake-Shake PyTorch Implementation

This repository contains a PyTorch implementation of **Shake-Shake Regularization**, a stochastic regularization method for **3-branch residual networks**. The goal is to improve generalization by **randomly mixing two residual branches** during training.

- Shake-Shake applied **within residual blocks** of the network.  
- Random Î± coefficients combine branch outputs during forward and backward passes.  
- Architecture:  
**Input â†’ Backbone with Shake-Shake â†’ Avg Pool â†’ Flatten â†’ FC â†’ Output**  

**Paper reference:** [Shake-Shake Regularization](https://arxiv.org/abs/1705.07485) ğŸ°


> ğŸ’¡ If you are interested in advanced regularization techniques, check out **[ShakeDrop-Replication]([https://github.com/your-link/ShakeDrop-Replication](https://github.com/barkinadiguzel/ShakeDrop-Replication))** for another interesting approach!
---

## ğŸ–¼ Overview â€“ Shake-Shake Architecture

![Figure 1](images/figmix.jpg)  

This overview summarizes the Shake-Shake network pipeline:

- **Input:** Original image enters the network.  
- **Backbone:** Feature maps are extracted using a ResNet-like backbone with Shake-Shake in residual blocks.  
- **Residual Mixing:** Two residual branches are combined with a random Î± coefficient per block.  
- **Pooling & FC:** Features are pooled, flattened, and fed into a fully connected layer for final classification.  

The model improves robustness by preventing over-reliance on any single residual branch.

---

## ğŸ§® Key Mathematical Idea

![Math Concept](images/math.jpg)  

Let $$x_i$$ be the input tensor to residual block $$i$$. Two residual branches with weights $$W_i^{(1)}$$ and $$W_i^{(2)}$$ produce:

$$
x_{i+1} = x_i + F(x_i, W_i^{(1)}) + F(x_i, W_i^{(2)})
$$

With Shake-Shake, a random variable $$\alpha_i \sim U(0,1)$$ mixes the branches during training:

$$
x_{i+1} = x_i + \alpha_i F(x_i, W_i^{(1)}) + (1-\alpha_i) F(x_i, W_i^{(2)})
$$

At test time, all $$\alpha_i$$ are set to the expected value 0.5.

---

## ğŸ—ï¸ Model Architecture

```bash
ShakeShake-Replication/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_block.py
â”‚   â”‚   â”œâ”€â”€ shake_layer.py
â”‚   â”‚   â”œâ”€â”€ residual_block.py
â”‚   â”‚   â”œâ”€â”€ downsample_block.py
â”‚   â”‚   â””â”€â”€ pool_fc_block.py
â”‚   â”‚
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â””â”€â”€ backbone_resnet.py
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ shake_resnet.py
â”‚   â”‚
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ figmix.jpg    
â”‚   â””â”€â”€ math.jpg       
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
